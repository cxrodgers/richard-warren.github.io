---
title: 'reinforcement learning: intro (part 1)'
date: 2020-4-2
permalink: /blog/rl_intro_1/
tags:
  - reinforcement learning
  - machine learning
read_time: false
---


While quarantined in NYC I've finally worked through [the classic text](http://incompleteideas.net/book/the-book-2nd.html) on reinforcement learning. This summary is intended for those interested in learning RL who are *not* interested in staying in their apartment for three months to learn it :mask:.


{% include toc %}
<br>

## overview

### motivation
Despite their neuro-inspired-namesakes, many modern deep learning algorithms can feel rather 'non-bioligcal'. Consider how living things learn. To teach my nephew the difference between cats and dogs, I *do not* show him thousands of cats and dogs until the (non-artificial) neural networks in his brain can distinguish them. This sort of training is well-suited to machines but not minds.

Moreover, much of what he learns is via *direct interaction with the world*. He knows what he likes and doesn't like, and through trial in error he maximizes the good stuff and minimizes the bad stuff. Although this isn't the only way animals learn (despite what [some psychologists used to think](https://en.wikipedia.org/wiki/Behaviorism)), it is a powerful approach to navigating the world.

Reinforcement learning turns this approach into powerful algorithms that are [sometimes](https://deepmind.com/research/case-studies/alphago-the-story-so-far) [superhuman](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning). [Sutton & Barto](http://incompleteideas.net/book/the-book-2nd.html) is the classic introductory text on the subject. The following is my summary of the text.


### setup
In reinforcement learning an **agent** collects **reward** by acting in an **environment**. The **actions** of the agent, together with the dynamics of the environment, determine how the **state** of the world changes and the amount of reward the agent gets. The goal is to get lots of reward by selecting good actions. Concretely, the agent has a **policy** $\pi(a|s)$ that maps states to actions. More specifically, $\pi(a|s)$ defines a *probability distribution* over actions conditioned on the state. We want to find a policy that gives us as much reward as possible.

At time $t$ the state of the environment is $S_t \in \mathcal{S}$ and an agent can take an action $A_t \in \mathcal{A}(s)$. The environment then emits a reward $R_{t+1} \in \mathbb{R}$ and a subsequent state, $S_{t+1}$. Notice how the reward is just a scalar. *From this sparse information the agent must learn to behave such that reward is maximized*. This should strike you as somewhat magical.

![agent environment interactions](/images/sutton_barto_notes/agent_environment.png){: .align-center}

The interactions between an agent, its actions, and the environment can be usefully modelled as a Markov Decision Process (MDP). In Markov systems the current state of the world tells you everything you need to know about what will happen next. In other words, *the future is independent of the past given the present*. Formally, $P(S_{t+1} \mid S_t) = P(S_{t+1} \mid S_0, S_1, \dots, S_{t})$. importantly, if the agent doesn't have complete information about the state of the world (i.e. it is *partially observed*), a markovian environment can be non-markovian from the perspective of the agent.

Given a state $s$ and action $a$, the probability of moving to a new state $s'$ with reward $r$ is given by the following. Note that this maps four arguments to a single value, $p : \mathcal{S} \times \mathcal{S} \times \mathcal{A} \times \mathcal{R} \rightarrow [0,1]$.

$$ p(s', r | s, a) = Pr(S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a) $$

The function $p(s',r \mid s,a)$ describes the dynamics of the environment. In **model-based** reinforcement learning we know (or learn) these dynamics, whereas in **model-free** methods we don't use them at all.

### return, return, return...
Our policy should not only encourage the acquisition of immediate reward, but also future reward. We therefore want to maximize **returns**, which are a function of *reward sequences*. For example:

$$ G_t = R_{t+1} + R_{t+2} + \dots + R_{T} $$

This is an undiscounted return because it equally weights all rewards. It is common to exponentially discount future rewards using a **discount factor** $\gamma$ (animals exhibit similar temporal discounting :mouse:):

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \dots = \sum_{k=0}^\infty \gamma^k R_{t+k+1} $$

Note that we can define returns recursively. Such recursive relationships are critical to many important ideas in reinforcement learning:

$$  
\begin{align}
G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \gamma^3R_{t+4} + \dots\\
& = R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \gamma^2R_{t+4} +\dots)\\
& = R_{t+1} + \gamma G_{t+1}
\end{align}
$$

### value functions
How can we maximize returns? A first thought might be to optimize the parameters of some policy with respect to the overall expected return (we'll get to these **policy gradient** methods later). An alternative approach is to learn how good different states are. We can then maximize returns by selecting actions that move us to the best states.

A **value function** describes how good different states are. Specifically, it tells us how much return we should expect in a given state:

$$
\begin{align*}
v_\pi (s) &= \mathbb{E}[G_t \mid S_t=s] \\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} \mid S_t=s] \\
&= \sum_a \pi (a|s) \sum_{s', r} p(s',r|s,a) [r + \gamma v_\pi (s')]
\end{align*}
$$

Let's unpack the third line. How valuable is a state? It depends on $(1)$ what action you take, $(2)$ the reward you get from that action, and $(3)$ the value of the state you end up in. $(1)$ means we must average over possible actions, weighted by their probabilities ($ \sum_a \pi (a \mid s) \dots $). $(2,3)$ mean we must average over possible states and rewards that could result from that action, again weighted by their probabilities ($ \sum_{s', r} p(s',r \mid s,a) \dots $).

We are therefore taking *expectations* over actions, rewards, and subsequent states. Note that $R_t$ and $G_t$ are *random variables*. The reward at a given time depends on the action selected, $A_t \sim \pi (a \mid s)$ and the (potentially) stochastic environment dynamics, $R_{t+1}, S_{t+1} \sim p(s',r \mid s,a)$. Therefore, when we query the value of given state, we must consider all possible actions, subsequent states, and subsequent rewards, each weighted by their probability. For this reason $\mathbb{E}[R_{t+1}] = \sum_a \pi (a \mid s) \sum_{s', r} p(s', r \mid s, a)r$.

The final line above is a **Bellman equation**, which recursively relates $v_\pi$ to itself. This Bellman equation states that the goodness of a state is the reward expected immediately (averaging across all actions), plus the discounted goodness of the subsequent states (averaging across all subsequent states for each action).

$v_\pi(s)$ is a **state-value function** because it reports the value of specific states. We will also consider **action-value** functions, which report the value of state-action pairs, i.e. the expected return conditioned on a particular state-action pair:

$$
\begin{align}
q_\pi(s,a) &= \mathbb{E}[G_t | S_t=s, A_t=a] \\
&= \sum_{s',r} p(s', r | s, a) [r + \gamma v(s')] & \scriptstyle{\text{average over potential $s',r$ resulting from action $a$}} \\
&= \sum_{s',r} p(s', r | s, a) [r + \gamma \sum_{a'} \pi(a' | s') q(s', a')] & \scriptstyle{\text{state-value is average over action-values}}
\end{align}
$$

An **optimal policy** yields the highest possible return. The optimal state-value and action-value functions are denoted $v_*$ and $q_*$, respectively. They can be recursively defined using the **Bellman optimality equations**:

$$
\begin{align}
v_*(s) &= \max_\pi v_\pi(s) \\
&= \max_a \sum_{s', r} p(s',r | s,a) [r + \gamma v_*(s')] \\
q_*(s,a) &= \max_\pi q_\pi(s,a) \\
&= \sum_{s', r} p(s',r | s,a) [r + \gamma \max_{a'} q_*(s', a')]
\end{align}
$$

Notice that the Bellman *optimality* equations take the max rather than the expectation over actions. This means that an optimal policy will always select actions that lead to the best possible subsequent state.


### backup diagrams
Many of these formulas can be intuited by drawing *backup diagrams*. In backup diagrams, states are represented by open circles and actions are black dots. Transitions from states (circles) to actions (dots) are governed by the policy $\pi(a \mid s)$. The transitions from actions (dots) to subsequent states (circles) are governed by the environment dynamics $p(s',r \mid s,a)$.

Note that for $v_{* }$ we only consider the action with the maximum return (maximizing denoted by the arc symbol). Also note that each action can result in several possible states and rewards; this is why we need to average over these branches. Similarly, for $q_{* }$ we already know the state and action, but we must average over the potential states an rewards resulting from that action, followed by taking the action that maximizes our subsequent $q_*$ value.

![backup diagram examples](/images/sutton_barto_notes/backup_diagrams.png){: .align-center}


## dynamic programming
We usually don't have complete access to the state of the world and its dynamics[^dynamics]. When we do, **dynamic programming**[^dp] can be used to iteratively converge on optimal policies. Generally, dynamic programming refers to algorithms in which a problem is decomposed into solvable sub-problems. If the same sub-problem is repeatedly encountered we can store and re-use its solution.

[^dynamics]: Down be thrown off by the word 'dynamics'. This is just $p(s',r \mid s,a)$, which describes how the world changes (in terms of its state and the rewards it emits) in response to the actions of the agent. It is just a probability distribution, which is not know in the model-free setting be we assume knowledge for model-based learning.

[^dp]: todo: exact vs approximate dynamic programming

Recall that the Bellman equations define a recursive relationship for value functions. We can therefore *bootstrap*, using the solution to a previous problem (figuring out the value of $S_{t+1}$) to solve our current problem (figuring out the value of $S_{t}$). With this logic we can turn the Bellman equations into recursive update rules that can be iteratively applied to converge on accurate value functions. These value functions can then be used to construct optimal policies.

Before considering how to improve our policy, let's first tackle **policy evaluation**, or **prediction**. This is the process of accurately determining the correct value function *for a particular policy*. We are not improving our policy yet; rather, we are trying to figure out how good each state is under the current policy.

In dynamic programming we do this by turning the Bellman equation for $v_\pi$ into a rule:

$$
\begin{aligned}
v_{k+1}(s) &= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] \\
&= \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a) [r + \gamma v_k(s')]
\end{aligned}
$$

In other words, $v_{\pi}$ is estimated as the reward we expect immediately plus the discounted value of the next state. Notice that *this approach is biased*. We are using an (initially) inaccurate estimate of subsequent states to update the value of the current state. Nonetheless, repeatedly applying this update (by looping over all states) lets us converge on the true value function. Awesome.

Now that we have an accurate value function we can update our policy. All we have to do is *act greedily with respect to it*. For example, if our policy says go left, but our value function says going right will lead to greater return, we update our policy accordingly.


### generalized policy iteration
So far so good - but wait! Our value function was defined with respect to $\pi_k$, but now we've updated the policy to $\pi_{k+1}$, which means our value function is no longer accurate :cry:. We therefore need to go back and update our value function again.

By repeating the process of updating our value function for a given policy (policy evaluation) and then updating our policy based on the new value function (policy improvement), we can converge on an optimal policy, $v_*$. This framework, which is common to many reinforcement learning algorithms, is know as **generalized policy iteration**.

In the following visualization, the upward arrows represent policy evaluation, wherein our value function *v* gets closer and closer to the true value function for the current policy, $v_\pi$. The downward arrows represent our policy improvement step, wherein we act greedily with respect to our update value function. By repeating this process we converge towards the optimal policy, $\pi_* $, and its associated value function $v_* $.

![generalized policy iteration](/images/sutton_barto_notes/gpi.png){: .align-center}


### value iteration
In the dynamic programming algorithm we just described, we must *iteratively* improve our value function after every policy update. How many times should we improve the value function for a given policy? Do we need to wait until it's nearly perfect? It turns out that even an *imperfect but improved* value function can help us find a better policy. In the extreme, we can update our value function *just once* for a given policy, and always act greedily with respect to that value function. This is called **value iteration**.

Let's see what this looks like algorithmically. To update our value function we apply our previous dynamic programming update rule just once for all states:

$$
v_{k+1} =  \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a) [r + \gamma v_k(s')] \,\,\, \forall \,\,\, s \in \mathcal{S}
$$

But wait! Because the policy is always greedy with respect to the current value function, $\pi(s \mid a)$ is zero for all but the best action, which has probability $1$. Therefore, rather than taking the expectation over actions we can simply take the max :sunglasses:. This gives us a nice, compact update rule (which should look suspiciously similar to the Bellman optimality equation):

$$ v_{k+1}(s) = \max_a \sum_{s',r} p(s',r|s,a) [r + \gamma v_k(s')] $$

Notice that the policy has disappeared from our equation. With this sleight of hand we are now working totally in 'value function' space. Only after our value function has converged do we set the policy to be greedy with respect to it.


### limitations of dynamic programming
This is sounds wonderful, but... Dynamic programming *requires that we know how the world works.* This is a lot to ask. To take expectations over actions and the states/rewards that result from those actions we need a model that tells us how the world reacts to what we do. Concretely, this means we need $p(s',r|s,a)$ to be given[^models].

Furthermore, to implement the dynamic programming algorithm described here we must loop over all states in the environment. This is feasible when the number of states is small (e.g. possible board configurations in tic-tac-toe), but not when the number of states is large (e.g. the possible board configurations in chess). We therefore need methods that work in the real world, which is big, scary, and full of states.

[^models]: Okay, $p(s',r \mid s,a)$ doesn't have to be given, but could be *learned* through interactions with the world. This super interesting topic is not covered in this post.

## monte carlo methods
How can we estimate the value function when we don't know the dynamics of the environment? Here's a simple but powerful idea. Recall that the value function is the expected return for a state (or state-action pair):

$$
\begin{aligned}
v_\pi(s) &= \mathbb{E}[G_t \mid S_t=s] &= {E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \dots] \\
q_\pi(s,a) &= \mathbb{E}[G_t \mid S_t=s, A_t=a] &= {E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \dots]
\end{aligned}
$$

**Monte carlo** methods estimate value functions by simply *averaging sample returns*. For example, $v(S_t)$ would simply be the average return we get following visits to state $S_t$. The estimates may be noisy (high variance) but will be correct on average (zero bias), and with enough samples they will be quite accurate. We can then find good policies by acting greedy with respect to these value functions.

### prediction
Let's make this concrete by describing a Monte Carlo prediction algorithm. First let's define an **episode** (or **rollout**, or **trajectory**) to be a series of states, actions, and rewards that ends in a terminal state at time $T$. Think of this as a single 'experience' the agent can learn from:

$$ S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_{T-1}, A_{T-1}, R_T $$

In the following algorithm we will generate episodes by following our policy. When we reach a state for the first time during an episode, we update its value to be the average return for that state across all episodes.

Annoyingly, we need *full trajectories* before updates can be performed (because returns are functions of all future rewards). We therefore wait until the end of an episode to update the values for visited states we visited. To do this we loop backwards in time, performing updates only for *first visits* to states in the trajectory (there are also *every visit* methods, which update with every visit to a state in the trajectory):

![image](/images/sutton_barto_notes/mc_prediction.png){: .align-center}

Notice that setting $G \leftarrow \gamma G + R_{t+1}$ when looping backwards from $T-1 \rightarrow 0$ causes $G_t$ to equal $R_{t+1} + \gamma R_{t+1} \gamma^2 R_{t+2} \dots \gamma^{T-1} R_{T}$.

### encouraging exploration
In dynamic programming we looped over all states to update our value function. In Monte Carlo we generate data by *letting our agent behave in the environment*. This is nice because updates to the value function will focus on states that are actually relevant to our policy. However, the agent may not explore the environment enough, which means very valuable states may never be reached.

We've hit upon a fundamental problem in reinforcement learning. The goal is to maximize returns. To do this, we take actions we *believe to be* the best. However, discovering new, better actions requires sometimes testing out actions we are unsure of. This tension between **exploitation** of what we know to be good and **exploration** to discover better actions is fundamental. A typical example is the decision to dine at your favorite restaurant (exploit) or try a new restaurant (explore)[^quarantine].

[^quarantine]: At the time and place of this writing (mid-COVID-quarantine in NYC) both options have pretty low value :sob:.

How can we ensure our agent tries potentially valuable actions that are not favored by the current policy? One approach is to adopt *stochastic* policies that usually pick the 'best' action, but sometimes select actions at random. So-called $\epsilon$*-greedy* policies select the 'best' action with probability $(1-\epsilon)$ and random actions with probability $\epsilon$. Equivalently, the non-greedy actions have probability $\frac{\epsilon}{ \vert \mathcal{A(s)} \vert }$ (where $ \vert \mathcal{A(s)} \vert $ is the number of possible actions in state $s$), whereas the greedy action has the remainder of the probability, $1 - \epsilon + \frac{\epsilon}{ \vert \mathcal{A(s)} \vert }$.

### control
The whole point of learning value functions is to do learn better policy, e.g. to do control. When the environment dynamics $p(s',r \mid s,a)$ are known, moving from the state value function $v_\pi$ to an improved policy is trivial; we simply select actions that lead to more valuable states ($p(s',r \mid s,a)$ is required to know the states an action leads to).

This is where the action-value function $q_\pi(s,a)$ comes in. If $q_\pi(s,a)$ is accurate, we don't need to know the how our state evolves as a function of our actions. All we need to do is pick the action that maximizes $q_\pi(s,a)$:

$$ A_t = \max_{a} q_\pi(S_t,a) $$

In the following algorithm we learn $q_\pi(s,a)$ using the first-visit Monte Carlo approach described above. Also, instead of always picking the 'best' action we use an $\epsilon$-greedy policy to encourage exploration:

![image](/images/sutton_barto_notes/mc_epsilon_soft.png){: .align-center}

### off-policy prediction
A very powerful way to ensure the state space is adequately explored is to have separate *behavior* and *target* policies. The behavior policy guides behavior during data acquisition whereas the target policy is actually being evaluated. Using an exploration-heavy behavior policy can encourage exploration of states that may be seldom visited under the target policy.

There's an important catch: if we average returns from trajectories sampled under the behavior policy, we will estimate expected returns under the behavior policy, whereas we want to evaluate the target policy :scream:. To address this, we can weight each return by its likelihood under target policy relative to the behavior policy. This weight is call an *importance sampling ratio* and is defined as:

$$
\rho_{t:T-1} = \prod_{k=t}^{T-1}\frac{\pi(A_k \mid S_k)}{b(A_k \mid S_k)}
$$

Here $b$ is the behavior policy and $\rho_{t:T-1}$ is the weight associated with the trajectory occurring between times $t$ and $T-1$. In weighting the returns by $\rho_{t:T-1}$, the expected value is the true value of the state under the target policy.

*Ordinary importance sampling* estimates the value of a state $s$ by averaging the returns from all visits to $s$, each weighted by $\rho_{t:T-1}$. In the following equation, $\mathcal{T}(s)$ is the set of all times state $s$ was visited and $T(s)$ is the time at which the episode containing time $t$ terminated. $V(s)$ is our estimate of the true value function, $v_\pi$.

$$ V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1} G_t} { \vert \mathcal{T}(s) \vert } $$

This approach has zero bias (it will converge to the desired expected value) but has very high variance. For example, if $\rho_{t:T-1}$ is large it can drastically overweight some returns. An alternative approach called *weighted importance sampling* uses a weighted average for trajectories. A weighted averaged means the weights must sum to 1. To accomplish this we divide by the sum of all $\rho_{t:T-1}$. This approach has lower variance but higher bias.

$$
V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1} G_t}    {\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1}}
$$

## temporal difference learning
Temporal difference learning is a model free approach in which we sample (like monte carlo) *and* bootstrap (like dynamic programming).

We update our value function by moving in the direction of the *estimated* return. The returns are estimated ($R_t + \gamma V(S_{t+1})$) rather than directly observed ($G_t$). Whereas in Monte Carlo we updated $V(S_t) \leftarrow V(S_t) + \alpha [{\color{blue}G_t} - V(S_t)]$, now we update using: $$\begin{aligned}
V(S_t)
&\leftarrow V(S_t) + \alpha [{\color{blue}R_{t+1} + \gamma V(S_{t+1})} - V(S_t)] \\
&= V(S_t) + \alpha [{\color{blue}\delta_t} - V(S_t)]\end{aligned}$$

$\delta_t$ is the *temporal difference error*, the difference between our bootstrapped estimate of the current return and our value estimate. TD learning has the advantage of being online; we don't have to wait until the end of our episode to update our value function. Furthermore, it reduces variance at the expense of increased bias (resulting from the value function initialization).

Monte Carlo and TD both converge to the true value function over time, but iteratively applying these methods to the same data reveals they are optimizing different things. Whereas Monte Carlo minimizes the mean squared error on the training data, TD finds the maximum-likelihood model for the underlying Markov process.

To illustrate, consider these sequences of states and rewards:

![image](reinforcement_learning/td_series.png){width=".4\linewidth"}

Monte Carlo gives a value of 0 to A, because every time we saw A there was a return of 0. However, (undiscounted) TD learning gives A a value of $\frac{6}{8}$, because A is always been followed by B, which itself has a value of $\frac{6}{8}$. TD has learned the maximum-likelihood MDP with the following structure:

![image](reinforcement_learning/td_series_model.png){width=".3\linewidth"}

### SARSA
r.075![image](reinforcement_learning/backup_sarsa.PNG){width="1\linewidth"}

SARSA learns action-value functions using an update rule analogous to the state-value update rule described above. Given a **S** tate, we select an **A** ction, observe the **R** eward / subsequent **S** tate, and then bootstrap our action-value function using our policy to select the next **A** ction: $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1}  + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$

### Q-learning
r.1![image](reinforcement_learning/backup_qlearning.PNG){width="1\linewidth"}

SARSA learns the action-value function associated with a given policy. The policy can then be improved by acting greedily with respect $q$. *Q-learning* is an off-policy algorithm that directly approximate $q_*$. To do this, we take the best action when bootstrapping, as opposed to the action chosen by our current policy: $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1}  + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$

### expected SARSA
r.15 ![image](reinforcement_learning/backup_expectedsarsa.PNG){width="1\linewidth"}

In SARSA, $A_{t+1}$ is a random variable that is selected according to our policy. This randomness introduces variance that can be mitigated by taking the expectation of subsequent actions:

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1}  + \gamma \sum_a \pi(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t)]$$


## $\lambda$ return
todo: something about striking balance between monte carlo and DP


### n-step bootstrapping
In Monte Carlo we sample entire trajectories. This method is low bias but has high variance due to sampling error. In TD(0) we sample one step and then bootstrap on our value function. This method has bias but much lower variance.

Although these approaches may appear qualitatively different, they are in fact the extremes of a continuum. In general we can take $n$ steps and then bootstrap. The number of steps determines the bias-variance trade-off (bigger $n \rightarrow$ less bias, more variance):

![image](reinforcement_learning/nstep.png){width=".7\linewidth"}

The $n$ step return and its associated update rule (for state-value functions) are: $$\begin{gathered}
G_{t:t+n} = R_{t+1} + \gamma  R_{t+2} + \gamma^2  R_{t+3} + \dots + \gamma^{n-1}  R_{t+n} + V(S_{t+n}) \\ \\
V(S_t) \leftarrow V(S_t) + \alpha [G_{t:t+n} - V(S_t)]\end{gathered}$$

Intuitively, taking larger $n$ allows us to see further back when assigning credit/blame to states and actions that led to reward. For example, upon reaching a reward state in TD(0) we only updated the value of the states/actions that caused reward, but the actions *that led to* the actions that caused reward are not updated. With $n=10$, however, we can assign credit to several states in the sequence that resulted in reward:

![image](reinforcement_learning/nstep_paths.png){width=".8\linewidth"}


### $\lambda$ return
$n$-step returns ($G_{t:t+n}$) allow us to control the extent to which we rely on sampling (big $n$) vs. bootstrapping (small $n$). Rather than picking a single $n$, we can take take an exponentially weighted average of $n$-step returns for all $n$: $$G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}G_{t:t+n}$$ $(1-\lambda)$ ensure the weights sum to 1. Note that for $\lambda=0$ this reduces to a one step return, whereas for $\lambda=1$ it is a monte carlo return. The weighting scheme is visualized as follows. Notice that all returns which reach the terminal state are collectively given the rest of the weight:

![image](reinforcement_learning/lambda_return.JPG){width=".6\linewidth"}

For continuous settings we can't compute $n$-step returns for arbitrarily large $n$. We can therefore approximate $G_t^\lambda$ by taking the first $k$ returns. By setting all $G_{t:t+k+\delta} =G_{t:t+k}$ for $\delta \in \mathbb{N}$ we give all the residual weight to $G_{t:t+k}$:

$$\begin{aligned}
G_{t:t+k}^\lambda
&= (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}G_{t:t+n} \\
&= (1-\lambda) \sum_{n=1}^{k-1} \lambda^{n-1}G_{t:t+n} + (1-\lambda) \sum_{n=k}^{\infty} \lambda^{n-1}G_{t:t+n} \\
&= (1-\lambda) \sum_{n=1}^{k-1} \lambda^{n-1}G_{t:t+n} + (1-\lambda) \sum_{n=k}^{\infty} \lambda^{n-1}G_{t:t+k} & \scriptstyle{\text{we are setting $G_{t:t+k+\delta} =G_{t:t+k} $ for $\delta \in \mathbb{N}$}}\\
&= (1-\lambda) \sum_{n=1}^{k-1} \lambda^{n-1}G_{t:t+n} + \lambda^{k-1}(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t:t+k} & \scriptstyle{\text{re-index and factor out $\lambda^{k-1}$}} \\
&= (1-\lambda) \sum_{n=1}^{k-1} \lambda^{n-1}G_{t:t+n} + \underbrace{\lambda^{k-1}G_{t:t+k}}_\text{residual weight} & \scriptstyle{\text{because $\sum_{n=1}^{\infty} \lambda^{n-1} = \frac{1}{1-\lambda}$}}\end{aligned}$$

### offline $\lambda$ return algorithm
The *offline $\lambda$ return algorithm* performs weight updates after each episode according to the $\lambda$ return at each time point using semi-gradient ascent as follows: $$\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left[G_t^\lambda - \hat{v}(S_t, \mathbf{w}_t) \right] \nabla_\mathbf{w} \hat{v}(S_t, \mathbf{w}_t)$$

### online $\lambda$ return algorithm
We can perform truncated $\lambda$ return updates online, but this requires a trick. If we are interested in $5$-step $\lambda$ returns, but we are only at $t=2$, we start by computing the $2$ -step return. Then when $t=3$, we go back and update our previous weight updates using the most recently available returns, and so on.

## forward vs. backward views
Value functions are expectations over returns, which are a function of *future* rewards. Every time we encounter a state, we update it's value in the direction of immediate rewards, and to a lesser extent distant rewards. We continue moving state by state, associating rewards with states based on their temporal proximity.

If a reward occurs soon *after* a state, we associate the reward with the state. By the same token, if a state occur soon *before* a reward, we should make the same association. This *backward view* is appealing because it opens the door to efficient, online algorithms for value function updates.

For the backward view to work, we need to keep track of how recently we visited each state. Just as rewards are discounted exponentially as time passes, our recency metric will fade exponentially. This recency metric is called as *eligibility trace*, $\mathbf{z}$. If we have a one-hot vector representation of the state, $\mathbf{x}$, then when we visit state $s$ we have $x_s \leftarrow x_s+1$ and $x_{i} \leftarrow \gamma \lambda x_i$. For each $x_i$ this causes decay patterns like this:

![image](reinforcement_learning/trace_decay.png){width=".7\linewidth"}

### TD($\lambda$)
TD($\lambda$) *approximates* the offline $\lambda$-return using an efficient backward view algorithm. We first need to generalize eligibility traces to function approximation. Recall that the general formulation for weight updates is: $$\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \nabla_\mathbf{w} \hat{v}(S_t, \mathbf{w}_t)$$ $\delta_t$ is the difference between the target and our current value estimate. This tells us how much we should be surprised by what is happening *now*. The associated update rule for the eligibility trace is: $$\mathbf{z}_{t} = \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{v}(S_t, \mathbf{w}_{t})$$ This more general eligibility trace no longer strictly reflects the recency of states. Rather, it reflects how elegible each element in $\mathbf{w}$ is for tweaking when something good or bad happens. It is like a smeared out version of the gradient. The new weight update is: $$\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t$$ At each time point, $\nabla \hat{v}(S_t, \mathbf{w}_{t})$ tells us how we would change each element in $\mathbf{w}$ to increase the value estimate of $S_t$. When things go better than expected, $\delta_t$ will be high. We then nudge $\mathbf{w}$ in the direction that increases the value of recent states, because $\mathbf{z}$ contains the gradients for past states exponentially weighted by their recency.

Notice that in the special case where $\mathbf{x}$ is a one-hot representation of the state and value function is linear, e.g. $\hat{v}(\mathbf{x}, \mathbf{w}) = \mathbf{w}^\top \mathbf{x}$, then $\nabla_\mathbf{w} \hat{v} = \mathbf{x}$ and the gradient based update rule is equivalent to the initial updated rule where 1 is added to only to the active state, and all other states decay.

We can construct SARSA($\lambda$) similarly by replacing the state-value function $v$ with the action-value function $q$.

### true TD($\lambda$) and dutch traces
TD($\lambda$) only approximates the offline $\lambda$-return. However, by tweaking the eligibility trace and weight update formulas, we can achieve a backward view algorithm that is equivalent to the online $\lambda$-return. The new *dutch trace* and associated weight update are: $$\begin{gathered}
\mathbf{z}_{t} = \gamma \lambda \mathbf{z}_{t-1} + (1 - \alpha \gamma \lambda\mathbf{z}_{t-1}^\top \mathbf{x}_{t})\mathbf{x}_{t} \\
\mathbf{w}_{t+1} = \mathbf{w}_{t} + \alpha \delta_t \mathbf{z}_{t} + \alpha (\mathbf{w}_{t}^\top\mathbf{x}_{t} - \mathbf{w}_{t-1}^\top\mathbf{x}_{t}) (\mathbf{z}_{t} - \mathbf{x}_{t})\\\end{gathered}$$ A proof for the equivalence of the forward and backward views using dutch traces is provided in the text for the case of monte carlo control without discounting (section 12.6).


## the big picture

### breadth vs. depth
The methods we've applied so far vary in breadth and depth. Methods that bootstrap (DP and TD) are shallow in that they only step a little into the future before bootstrapping. Monte Carlo is deep in that it samples until the end of trajectories. Furthermore, methods that take expectations over actions are wide, such as DP, whereas both TD and Monte Carlo can be thought of as narrow because they rely on sampling rather than averaging over actions.

Methods exist in between these extremes and can be thought of as living in a space characterized by two axes. These axes represent the extent to which a method relies on sampling vs. expectations (narrow vs. wide), and bootstrapping vs. sampling (shallow vs. deep):

![image](reinforcement_learning/rl_bigpicture.PNG){width=".6\linewidth"}

### value function updates
The value function updates we've considered differ in whether they 1) update the state or action value function, 2) approximate the optimal or an arbitrary policy, and 3) rely on sample or expected updates. The different combinations of these three binary dimensions yield the following family of value function updates:

![image](reinforcement_learning/value_updates.PNG){width=".5\linewidth"}
